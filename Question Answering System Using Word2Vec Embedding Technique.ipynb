{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8442,
     "status": "ok",
     "timestamp": 1619419790081,
     "user": {
      "displayName": "SATYAM SONI",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhG4rcPYa7hnDqGvLTYPIBiXe9m-k4IPTmu9dWD=s64",
      "userId": "06127152709350492896"
     },
     "user_tz": -330
    },
    "id": "DvP8WnJHfNVb",
    "outputId": "6b7dfde8-deb8-4e8a-aa4c-45465757db58",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install -q memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3941,
     "status": "ok",
     "timestamp": 1619419796769,
     "user": {
      "displayName": "SATYAM SONI",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhG4rcPYa7hnDqGvLTYPIBiXe9m-k4IPTmu9dWD=s64",
      "userId": "06127152709350492896"
     },
     "user_tz": -330
    },
    "id": "sTCcRsKkfNP3",
    "outputId": "2cb8206a-ce93-47be-9c27-7cdc969aecaa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2E2YFFZxfNSx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "pdf = pdfplumber.open('A brief History of Modern India - Rajiv Ahir.pdf')\n",
    "c = 0\n",
    "pdf_txt = \"\"\n",
    "flag = True\n",
    "while flag == True:\n",
    "    try:\n",
    "        # print(c)\n",
    "        page = pdf.pages[c]\n",
    "        c = c+1\n",
    "        pdf_txt = pdf_txt + page.extract_text()\n",
    "    except:\n",
    "        pdf.close()\n",
    "        flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t.me/Ebooks_Encyclopedia27. t.me/Magazines4all\n",
      "A Brief History\n",
      "of Modern India\n",
      "by\n",
      "Rajiv Ahir\n",
      "I.P.S.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pdf_txt[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ashishsingh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lwHObl-Sj-ZJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "def clean_sentence(sentence, stopwords=False):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r'[^a-z0-9\\s]', '', sentence)\n",
    "    sentence = re.sub(r'\\n', ' ', sentence)\n",
    "    if stopwords:\n",
    "        sentence = remove_stopwords(sentence)\n",
    "    return sentence\n",
    "\n",
    "def get_cleaned_sentences(tokens, stopwords=False):\n",
    "    cleaned_sentences = []\n",
    "    for row in tokens:\n",
    "        cleaned = clean_sentence(row, stopwords)\n",
    "        cleaned_sentences.append(cleaned)\n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 819078,
     "status": "ok",
     "timestamp": 1619421436799,
     "user": {
      "displayName": "SATYAM SONI",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhG4rcPYa7hnDqGvLTYPIBiXe9m-k4IPTmu9dWD=s64",
      "userId": "06127152709350492896"
     },
     "user_tz": -330
    },
    "id": "2wtseJzQgz7w",
    "outputId": "17fc54ca-7e42-42a6-d12d-65b859f9ea52",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "def load_model():\n",
    "    v2w_model = None\n",
    "    try:\n",
    "        v2w_model = KeyedVectors.load('./w2vecmodel.mod')\n",
    "        print(\"w2v Model Successfully loaded\")\n",
    "    except:\n",
    "        v2w_model = api.load('word2vec-google-news-300')\n",
    "        v2w_model.save(\"./w2vecmodel.mod\")\n",
    "        print(\"w2v Model Saved\")\n",
    "\n",
    "    w2vec_embedding_size = len(v2w_model['pc'])\n",
    "    \n",
    "    return v2w_model, w2vec_embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "swnbdWppgz4s",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getWordVec(word, model):\n",
    "    samp = model['pc']\n",
    "    vec = [0]*len(samp)\n",
    "    try:\n",
    "        vec = model[word]\n",
    "    except:\n",
    "        vec = [0]*len(samp)\n",
    "    return (vec)\n",
    "\n",
    "\n",
    "def getPhraseEmbedding(phrase, embeddingmodel):\n",
    "    samp = getWordVec('computer', embeddingmodel)\n",
    "    vec = np.array([0]*len(samp))\n",
    "    den = 0;\n",
    "    for word in phrase.split():\n",
    "        den = den+1\n",
    "        vec = vec+np.array(getWordVec(word, embeddingmodel))\n",
    "    return vec.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def retrieveAndPrintFAQAnswer(question_embedding, sentence_embeddings, sentences):\n",
    "    max_sim = -1\n",
    "    index_sim = -1\n",
    "    for index, embedding in enumerate(sentence_embeddings):\n",
    "        sim = cosine_similarity(embedding, question_embedding)[0][0]\n",
    "        # print(index, sim, sentences[index])\n",
    "        if sim > max_sim:\n",
    "            max_sim = sim\n",
    "            index_sim = index\n",
    "  \n",
    "    return index_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "90Ua1NIogzrL"
   },
   "outputs": [],
   "source": [
    "def train_predict_pipeline(question):\n",
    "    raw_sentences = nltk.sent_tokenize(pdf_txt)\n",
    "    cleaned_sentences = get_cleaned_sentences(raw_sentences, stopwords=True)\n",
    "    cleaned_sentences_with_stopwords = get_cleaned_sentences(raw_sentences, stopwords=False)\n",
    "    \n",
    "    v2w_model, w2vec_embedding_size = load_model()\n",
    "    \n",
    "    sent_embeddings = []\n",
    "    for sent in cleaned_sentences:\n",
    "        sent_embeddings.append(getPhraseEmbedding(sent, v2w_model))\n",
    "    \n",
    "    question = clean_sentence(question, stopwords=True)\n",
    "    question_embedding = getPhraseEmbedding(question, v2w_model)\n",
    "    index = retrieveAndPrintFAQAnswer(question_embedding, sent_embeddings, cleaned_sentences_with_stopwords)\n",
    "    \n",
    "    print(\"Question: \", question)\n",
    "    print(\"Answer: \", cleaned_sentences_with_stopwords[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v Model Successfully loaded\n",
      "Question:  pilot led vasco da gama ships\n",
      "Answer:  from trading to ruling vasco da gama the arrival of three ships under vasco da gama led by a gujarati pilot named abdul majid at calicut in may 1498 profoundly affected the course of indian history\n",
      "CPU times: user 2.73 s, sys: 1.32 s, total: 4.04 s\n",
      "Wall time: 4.93 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"Which pilot led the Vasco Da Gama ships ?\"\n",
    "\n",
    "%time train_predict_pipeline(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v Model Successfully loaded\n",
      "Question:  pilot led vasco da gama ships\n",
      "Answer:  from trading to ruling vasco da gama the arrival of three ships under vasco da gama led by a gujarati pilot named abdul majid at calicut in may 1498 profoundly affected the course of indian history\n",
      "peak memory: 3867.81 MiB, increment: 3301.19 MiB\n"
     ]
    }
   ],
   "source": [
    "%reload_ext memory_profiler\n",
    "%memit train_predict_pipeline(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v Model Successfully loaded\n",
      "Question:  marxist approach\n",
      "Answer:  marxist historiography approach the beginning of the marxist approach in india was heralded by two classic booksrajni palme dutts india today and ar\n",
      "CPU times: user 2.74 s, sys: 1.25 s, total: 4 s\n",
      "Wall time: 5.24 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"What is Marxist Approach ?\"\n",
    "\n",
    "%time train_predict_pipeline(question)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOmUnlVA5ba2hwPlk4wgGWT",
   "collapsed_sections": [],
   "name": "Question Answering System Using Word2Vec Embedding Technique.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
